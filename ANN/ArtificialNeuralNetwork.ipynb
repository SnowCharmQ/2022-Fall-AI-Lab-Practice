{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957594d9",
   "metadata": {},
   "source": [
    "# Practice 11: Artificial Neural Network\n",
    "In this lab, you need to implement a 4-layer artificial neural network and test your model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920bad6",
   "metadata": {},
   "source": [
    "### Part 1: Artificial Neural Network\n",
    "Artificial Neural Network (ANN) consists of multiple artificial neurons, usually having the structure of an input layer, multiple hidden layer, an output layer. **Layers** in the artificial neural network consist of **neurons**.\n",
    "\n",
    "Suppose that we have a N-layer ($N\\geq2$) ANN. The first layer is the input layer. The last layer is the output layer. For $n$-th layer ($n\\geq2$) with $d_n$ nodes, given the input from the last layer $x^{(n-1)}_i$ with $d_{n-1}$ nodes, the output of this layer is:\n",
    "$$x^{(n)}_i=g(a^{n})=g(w^{(n)}x^{(n-1)}+b^{(n)}),$$\n",
    "where $g$ is the activation function, $w^{n}$ is the weights from layer $n-1$ to layer $n$ with a size of $(d_n*d_{n-1})$, and $b^{(n)}$ is the bias.\n",
    "\n",
    "For example, in a 4-layer ANN, the output of each layer is processing in this way:\n",
    "$$\\mathbf{x} = \\mathbf{x}^{(1)} \\rightarrow \\mathbf{x}^{(2)} \\rightarrow \\mathbf{x}^{(3)}\\rightarrow \\mathbf{x}^{(4)} = \\hat{y}$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Given a train dataset $\\mathbb{D}_{train}=\\{(\\mathbf{x}_i, y_i)|i \\in [1,m], \\mathbf{x}_i \\in \\mathbb{R}^d, y_i \\in \\{1, 2, ..., K\\}\\}$. The cross-entropy loss function is:\n",
    "$$Loss=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}-t_i^k log \\hat{y}^k_i + \\frac{\\lambda}{2}||W||^2_2,$$\n",
    "where $-t_i^k$ is the $k$-th value of the one-hot vector of $y_i$, $\\hat{y}^k_i$ is the $k$-th value of the output from the ANN, and $\\lambda$ is the weight of the regularization term.\n",
    "\n",
    "To minimize the loss function, we will conduct batch gradient descent algorithm. For the $n$-th layer ($n\\geq2$) in a $N$-layer ($N\\geq2$) ANN, the gradient of the loss function on weight matrix $w^{(n)}$ is:\n",
    "$$\\frac{\\partial Loss}{w^{(n)}} = \\frac{\\partial Loss}{\\partial x^{(N)}}\\frac{\\partial x^{(N)}}{\\partial x^{(N-1)}}...\\frac{\\partial x^{(n+1)}}{\\partial x^{(n)}}\\frac{\\partial x^{(n)}}{\\partial w^{(n)}}.$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "In this work, we will use relu activation function in hidden layers and the softmax function in the output layer.\n",
    "* The gradient on weigths of the output layer is: (derivation process can be found in <https://deepnotes.io/softmax-crossentropy>)\n",
    "$$\\frac{\\partial Loss}{w^{(N)}} = \\frac{\\partial Loss}{\\partial a^{(N)}}\\frac{\\partial (a^{N})}{\\partial w^{(N)}}= \\frac{1}{m}\\sum_{i=1}^{m}(x^{(N-1)}_i)^T*(\\hat{y}_i-y_i)$$\n",
    "let $\\delta_N=\\frac{\\partial Loss}{\\partial a^{(N)}}=(\\hat{y}_i-y_i)$, then \n",
    "$$\\frac{\\partial Loss}{w^{(N)}} = \\frac{1}{m}\\sum_{i=1}^{m}\\delta_N(x^{(N-1)}_i)^T+\\lambda w^{(N)}$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(N)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_N$$\n",
    "\n",
    "* For the penultimate layer $N-1$ (derivation process can be found in <https://sudeepraja.github.io/Neural/>):\n",
    "$$\\frac{\\partial Loss}{w^{(N-1)}} = \\frac{\\partial Loss}{\\partial a^{(N-1)}}\\frac{\\partial a^{(N-1)}}{\\partial w^{(N-1)}} = \\frac{1}{m}\\sum_{i=1}^{m}(x^{(N-2)}_i)^T\\delta_{N-1},$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(N-1)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{N-1}$$\n",
    "where$$\\delta_{N-1}=\\frac{\\partial Loss}{\\partial a^{(N-1)}}=\\delta_{N}(w^{(N)})^T\\circ\\frac{\\partial g(a^{(N-1)})}{\\partial a^{(N-1)}}$$\n",
    "\n",
    "* So, for the $n$-th layer ($2\\leq n\\leq N-1$):\n",
    "$$\\frac{\\partial Loss}{w^{(n)}} = \\frac{\\partial Loss}{\\partial a^{(n)}}\\frac{\\partial a^{(n)}}{\\partial w^{(n)}} = \\frac{1}{m}\\sum_{i=1}^{m}(x^{(n-1)}_i)^T\\delta_{n}+ \\lambda w^{(n)},$$\n",
    "$$\\frac{\\partial Loss}{\\partial b^{(n)}}= \\frac{1}{m}\\sum_{i=1}^{m}\\delta_{n}$$\n",
    "where$$\\delta_{n}=\\frac{\\partial Loss}{\\partial a^{(n)}}=\\delta_{n+1}(w^{(n+1)})^T\\circ\\frac{\\partial g(a^{(n)})}{\\partial a^{(n)}}.$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "Based on above derivations, you need to complete the missing code in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679afb9",
   "metadata": {},
   "source": [
    "### Part 2: Design a ANN for MNIST dataset\n",
    "\n",
    "MNIST stand for Mixed National Institute of Standards and Technology, whch has produced a handwritten digits dataset. This dataset is one of the most popular datasets in machine learning. \n",
    "\n",
    "The overall all dataset contains 60000 training images and 10000 testing images with 10 classes from 0 to 9, formmatted as $28*28$ pixel monochrome images.\n",
    "\n",
    "![MNIST](./MNIST.png)\n",
    "\n",
    "The mnist is already downloaded in the file `data.pkl`. Let's import the data and standarize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2414053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "(train_images, train_labels,\n",
    " test_images, test_labels) = pickle.load(open('data.pkl', 'rb'), encoding='latin1')\n",
    "\n",
    "### normalize all pixels to [0,1) by dividing 256\n",
    "train_images = train_images / 256.0\n",
    "test_images = test_images / 256.0\n",
    "print(type(train_images[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6ea0b",
   "metadata": {},
   "source": [
    "Next, we need to design an ANN to conduct the classifcation task in MNIST dataset. As shown in PPT, this ANN have your layers:\n",
    "* Layer 1: input layer, size 784 = $28*28$\n",
    "* Layer 2: Hidden layer, size = 300, activation = relu\n",
    "* Layer 3: Hidden layer, size = 100, activation = relu\n",
    "* Layer 4: Output layer , size = 10, activation = softmax\n",
    "![ANN](./ANN.png)\n",
    "You need to complete the following parts:\n",
    "1. Initialize the all parameters in ANN\n",
    "2. Complete the forward process of hidden layer\n",
    "3. Complete the forward process of output layer\n",
    "4. Complete the loss calculation function\n",
    "5. Calculate the gradient on these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a3790a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def xavier(n1, n2):\n",
    "    \"\"\"\n",
    "    Initialize a matrix with a shape of (n1, n2)\n",
    "    \n",
    "    :param n1: int, number of rows of the matrix\n",
    "    :param n2: int, number of columns of the matrix\n",
    "    \n",
    "    :return:\n",
    "        ndarray: numpy.ndarray, a random matrix generated by the uniform distribution.\n",
    "    \"\"\"\n",
    "    x = np.sqrt(6 * 1.0 / (n1 + n2))\n",
    "    ndarray = np.random.uniform(low=-x, high=x, size=(n1, n2))\n",
    "\n",
    "    return ndarray\n",
    "\n",
    "\n",
    "def softmax(ndarray):\n",
    "    \"\"\"\n",
    "    Softmax function for input with a shape of (batch_size, n_classes)\n",
    "    \n",
    "    :param ndarray: numpy.ndarray, the output of the last layer in ANN with a shape of (batch_size, n_classes)\n",
    "    \n",
    "    :return:\n",
    "        numpy.ndarray, the softmax output with a shape of (batch_size, n_classes)\n",
    "    \"\"\"\n",
    "    r = ndarray.shape[0]\n",
    "    c = ndarray.shape[1]\n",
    "    max_arr = np.tile(ndarray.max(axis=1).reshape(r, 1), (1, c))\n",
    "    e_x = np.exp(ndarray - max_arr)\n",
    "    return e_x / np.tile(e_x.sum(axis=1).reshape(r, 1), (1, c))\n",
    "\n",
    "\n",
    "def relu(ndarray):\n",
    "    return np.maximum(ndarray, 0)\n",
    "\n",
    "\n",
    "def d_relu(ndarray):\n",
    "    ndarray[ndarray > 0] = 1\n",
    "    ndarray[ndarray <= 0] = 0\n",
    "    return ndarray\n",
    "\n",
    "\n",
    "def get_label(ndarray, BATCHSIZE, num_output):\n",
    "    label = np.zeros((BATCHSIZE, num_output))\n",
    "    for i in range(0, BATCHSIZE):\n",
    "        idx = ndarray[i]\n",
    "        label[i][idx] = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "833a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_initialization(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    \"\"\"\n",
    "    Parameter initialization function for our ANN.\n",
    "    \n",
    "    :param input_size: int\n",
    "    :param hidden1_size: int\n",
    "    :param hidden2_size: int\n",
    "    :param output_size: int\n",
    "    \n",
    "    :return:\n",
    "        Parameter: dict, a dictionary to store the parameters in ANN.\n",
    "    \"\"\"\n",
    "    # TODO 1:\n",
    "    # Please Initialize all parameters used in ANN-Hidden Layers with Xavier\n",
    "    parameters = {}\n",
    "\n",
    "    #Your code starts here\n",
    "    w2 = xavier(hidden1_size, input_size)\n",
    "    b2 = xavier(hidden1_size, 1)\n",
    "    w3 = xavier(hidden2_size, hidden1_size)\n",
    "    b3 = xavier(hidden2_size, 1)\n",
    "    w4 = xavier(output_size, hidden2_size)\n",
    "    b4 = xavier(output_size, 1)\n",
    "    #Your code ends here\n",
    "\n",
    "    parameters['w2'] = w2\n",
    "    parameters['b2'] = b2\n",
    "\n",
    "    parameters['w3'] = w3\n",
    "    parameters['b3'] = b3\n",
    "\n",
    "    parameters['w4'] = w4\n",
    "    parameters['b4'] = b4\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def hidden_layer(x, w, b):\n",
    "    \"\"\"\n",
    "    Return the output of the hidden layer.\n",
    "\n",
    "    :param x: numpy.ndarray with a shape of (n, input_dim), input data of this layer.\n",
    "    :param w: numpy.ndarray, weights of this layer.\n",
    "    :param b: numpy.ndarray, bias of this layer.\n",
    "    \n",
    "    :return:\n",
    "        z: numpy.ndarray with a shape of (n, output_dim), output data of this layer\n",
    "    \"\"\"\n",
    "    # TODO 2: generate the outputs of this layer\n",
    "    # Your code starts here\n",
    "    n = x.shape[0]\n",
    "    in_dim = x.shape[1]\n",
    "    out_dim = b.shape[0]\n",
    "    z = np.zeros((n, out_dim))\n",
    "    for i in range(n):\n",
    "        tmp = x[i].reshape((in_dim, 1))\n",
    "        tmp = np.matmul(w, tmp)\n",
    "        z[i] = (tmp + b).reshape((out_dim, ))\n",
    "    # Your code ends here\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def output_layer(x, w, b):\n",
    "    \"\"\"\n",
    "    Return the output of the output layer.\n",
    "\n",
    "    :param x: numpy.ndarray with a shape of (n, input_dim), input data of this layer.\n",
    "    :param w: numpy.ndarray, weights of this layer.\n",
    "    :param b: numpy.ndarray, bias of this layer.\n",
    "    \n",
    "    :return:\n",
    "        z: numpy.ndarray with a shape of (n, num_classes), output of the output layer\n",
    "    \"\"\"\n",
    "    # TODO 3: generate the outputs of this layer\n",
    "    # Your code starts here\n",
    "    n = x.shape[0]\n",
    "    in_dim = x.shape[1]\n",
    "    out_dim = b.shape[0]\n",
    "    z = np.zeros((n, out_dim))\n",
    "    for i in range(n):\n",
    "        tmp = x[i].reshape((in_dim, 1))\n",
    "        tmp = np.matmul(w, tmp)\n",
    "        z[i] = (tmp + b).reshape((out_dim, ))\n",
    "    # Your code ends here\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def loss(label, output):\n",
    "    \"\"\"\n",
    "    Return the loss of ANN.\n",
    "\n",
    "    :param label: numpy.ndarray with a shape of (n, num_classes), true labels (each line is a one-hot vector)\n",
    "    :param output: numpy.ndarray with a shape of (n, num_classes), predicted results of your model\n",
    "    \n",
    "    :return:\n",
    "        cross_entropy_loss: float, \n",
    "    \"\"\"\n",
    "    # TODO 4: calculate the loss of the output\n",
    "    # Your code starts here\n",
    "    m, k = label.shape\n",
    "    tmp = - label * np.log(np.abs(output))\n",
    "    cross_entropy_loss = np.sum(tmp) / m\n",
    "    # Your code ends here\n",
    "\n",
    "    return cross_entropy_loss\n",
    "\n",
    "\n",
    "def bp_output(predict, label, w, x, lam):\n",
    "    \"\"\"\n",
    "    Return the gradients of loss on the weight of output layer, and the delta_N\n",
    "    \n",
    "    :param predict: numpy.ndarray with a shape of (n, num_classes), predicted results of your model\n",
    "    :param label: numpy.ndarray with a shape of (n, num_classes), true labels (each line is a one-hot vector)\n",
    "    :param w: weight matrix from hidden layer 2 to output layer\n",
    "    :param x: input of the output layer\n",
    "    :param lam: the weight of the regularization term\n",
    "    \n",
    "    :return:\n",
    "        grad_w: gradient of loss on the weight of output layer\n",
    "        grad_b: gradient of loss on the bias of output layer\n",
    "        delta_N: partial loss / partial a_N\n",
    "    \"\"\"\n",
    "    # TODO 5: calculate the gradients of loss on the weight of output layer\n",
    "    grad_w, grad_b, delta_n = 0, 0, 0\n",
    "\n",
    "    return grad_w, grad_b, delta_n\n",
    "\n",
    "\n",
    "def bp_hidden(delta_n_1, w_n_1, w_n, b_n, x, lam):\n",
    "    \"\"\"\n",
    "    Return the gradients of loss on the weight of layer n and the delta_n.\n",
    "    \n",
    "    :param delta_n_1: numpy.ndarray with a shape of (n, size_layer(n+1)), delta of the layer n+1\n",
    "    :param w_n_1: weight matrix of layer n+1 with a shape of (size_layer(n), size_layer(n+1))\n",
    "    :param w_n: weight matrix of layer n with a shape of (size_layer(n-1), size_layer(n))\n",
    "    :param b_n: bias of layer n with a shape of (1, size_layer(n))\n",
    "    :param x: input of layer n with a shape of (n, size_layer(n-1))\n",
    "    :param lam: the weight of the regularization term\n",
    "    \n",
    "    :return:\n",
    "        grad_w: gradient of loss on the weight of current hidden layer n\n",
    "        grad_b: gradient of loss on the bias of current hidden layer n\n",
    "        delta_n: partial loss / partial a_n\n",
    "    \"\"\"\n",
    "    # TODO 6: calculate the gradients of loss on the weight of output layer  \n",
    "    grad_w, grad_b, delta_n = 0, 0, 0\n",
    "    return grad_w, grad_b, delta_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50417942",
   "metadata": {},
   "source": [
    "After finishing the ANN codes, let's test it on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c95ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hyper parameters\n",
    "EPOCH = 100\n",
    "ITERS = 100\n",
    "BATCHSIZE = 100\n",
    "LR_BASE = 0.1\n",
    "lam = 0.0005  # lambda\n",
    "num_input = 784\n",
    "num_layer1 = 300\n",
    "num_layer2 = 100\n",
    "num_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afe7a90b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: \n",
      "1.065276603775272\n",
      "Accuracy: \n",
      "0.06899999999999995\n",
      "Epoch 2: \n",
      "1.065276603775272\n",
      "Accuracy: \n",
      "0.06899999999999995\n",
      "Epoch 3: \n",
      "1.065276603775272\n",
      "Accuracy: \n",
      "0.06899999999999995\n",
      "Epoch 4: \n",
      "1.065276603775272\n",
      "Accuracy: \n",
      "0.06899999999999995\n",
      "Epoch 5: \n",
      "1.065276603775272\n",
      "Accuracy: \n",
      "0.06899999999999995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [20], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m x1 \u001B[38;5;241m=\u001B[39m image_blob\n\u001B[0;32m     20\u001B[0m x2 \u001B[38;5;241m=\u001B[39m hidden_layer(x1, w2, b2)\n\u001B[1;32m---> 21\u001B[0m x3 \u001B[38;5;241m=\u001B[39m \u001B[43mhidden_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb3\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Forward propagation  output Layer\u001B[39;00m\n\u001B[0;32m     23\u001B[0m x4 \u001B[38;5;241m=\u001B[39m output_layer(x3, w4, b4)\n",
      "Cell \u001B[1;32mIn [18], line 54\u001B[0m, in \u001B[0;36mhidden_layer\u001B[1;34m(x, w, b)\u001B[0m\n\u001B[0;32m     52\u001B[0m in_dim \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     53\u001B[0m out_dim \u001B[38;5;241m=\u001B[39m b\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 54\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n):\n\u001B[0;32m     56\u001B[0m     tmp \u001B[38;5;241m=\u001B[39m x[i]\u001B[38;5;241m.\u001B[39mreshape((in_dim, \u001B[38;5;241m1\u001B[39m))\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 2. Weight initialization: Xavier\n",
    "parameters = parameter_initialization(num_input, num_layer1, num_layer2, num_output)\n",
    "w2, w3, w4 = parameters['w2'], parameters['w3'], parameters['w4']\n",
    "b2, b3, b4 = parameters['b2'], parameters['b3'], parameters['b4']\n",
    "\n",
    "# 3. training of neural network\n",
    "losses = np.zeros(EPOCH)  #save the loss of each epoch\n",
    "accuracy = np.zeros(EPOCH)  #save the accuracy of each epoch\n",
    "for epoch in range(0, EPOCH):\n",
    "    if epoch <= EPOCH / 2:\n",
    "        lr = LR_BASE\n",
    "    else:\n",
    "        lr = LR_BASE / 10.0\n",
    "    for iters in range(0, ITERS):\n",
    "        image_blob = train_images[iters * BATCHSIZE:(iters + 1) * BATCHSIZE, :]  # 100*784\n",
    "        label_blob = train_labels[iters * BATCHSIZE:(iters + 1) * BATCHSIZE]  # 100*1\n",
    "        label = get_label(label_blob, BATCHSIZE, num_output)\n",
    "        # Forward propagation  Hidden Layer\n",
    "        x1 = image_blob\n",
    "        x2 = hidden_layer(x1, w2, b2)\n",
    "        x3 = hidden_layer(x2, w3, b3)\n",
    "        # Forward propagation  output Layer\n",
    "        x4 = output_layer(x3, w4, b4)\n",
    "        y_hat = x4\n",
    "        assert np.count_nonzero(y_hat) == 1000\n",
    "        #comupte loss\n",
    "        loss_tmp = loss(label, y_hat)\n",
    "\n",
    "        if iters % 100 == 99:\n",
    "            losses[epoch] = loss_tmp\n",
    "            print('Epoch ' + str(epoch + 1) + ': ')\n",
    "            print(loss_tmp)\n",
    "        # Back propagation\n",
    "        grad_w4, grad_b4, delta_4 = bp_output(y_hat, label, w4, x3, lam)  # output layer\n",
    "        grad_w3, grad_b3, delta_3 = bp_hidden(delta_4, w4, w3, b3, x2, lam)  # hidden layer 2\n",
    "        grad_w2, grad_b2, delta_2 = bp_hidden(delta_3, w3, w2, b2, x1, lam)  # hidden layer 1\n",
    "\n",
    "        # Gradient update\n",
    "        w2 = w2 - lr * grad_w2\n",
    "        w3 = w3 - lr * grad_w3\n",
    "        w4 = w4 - lr * grad_w4\n",
    "        b2 = b2 - lr * grad_b2\n",
    "        b3 = b3 - lr * grad_b3\n",
    "        b4 = b4 - lr * grad_b4\n",
    "\n",
    "        # Testing for accuracy\n",
    "        if iters % 100 == 99:\n",
    "            x1 = test_images\n",
    "            x2 = hidden_layer(x1, w2, b2)\n",
    "            x3 = hidden_layer(x2, w3, b3)\n",
    "            # Forward propagation  output Layer\n",
    "            x4 = hidden_layer(x3, w4, b4)\n",
    "            y_hat = x4\n",
    "\n",
    "            predict = np.argmax(y_hat, axis=1)\n",
    "            print('Accuracy: ')\n",
    "            accuracy[epoch] = 1 - np.count_nonzero(predict - test_labels) * 1.0 / 1000\n",
    "            print(accuracy[epoch])\n",
    "\n",
    "### 4. Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(np.arange(EPOCH) + 1, loss[0:], 'r', label='Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss on trainSet', fontsize=16)\n",
    "plt.grid()\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(np.arange(EPOCH) + 1, accuracy[0:], 'b', label='Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy on trainSet', fontsize=16)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
